{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77cc7fff",
   "metadata": {},
   "source": [
    "# üìù EfficientDet_pytorch_Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7806df30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\r\n",
      "#\r\n",
      "base                     /home/minki_kwak01/anaconda3\r\n",
      "minki                 *  /home/minki_kwak01/anaconda3/envs/minki\r\n",
      "mmdetection              /home/minki_kwak01/anaconda3/envs/mmdetection\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# Í∞ÄÏÉÅÌôòÍ≤Ω ÌôïÏù∏\n",
    "!conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "576116d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mminkikwak\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/minki_kwak01/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wandb Î°úÍ∑∏Ïù∏\n",
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5e96a6",
   "metadata": {},
   "source": [
    "## ÌïôÏäµ ÏßÑÌñâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc7faa42",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.43s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.06s)\n",
      "creating index...\n",
      "index created!\n",
      "using weights EfficientDet_ckpt/EfficientDet_d0/efficientdet-d0_47_96000.pth\n",
      "[Info] loaded weights: efficientdet-d0_47_96000.pth, resuming checkpoint from step: 96000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mminkikwak\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/minki_kwak01/minki/EfficientDet/Yet-Another-EfficientDet-Pytorch/wandb/run-20221208_160549-3o8hjwh8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mh_default_epoch_100\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/minkikwak/efficientdet-d0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/minkikwak/efficientdet-d0/runs/3o8hjwh8\u001b[0m\n",
      "Step: 97999. Epoch: 48/100. Iteration: 2000/2000. Cls loss: 0.30045. Reg loss: 0checkpoint...\n",
      "Step: 97999. Epoch: 48/100. Iteration: 2000/2000. Cls loss: 0.30045. Reg loss: 0\n",
      "Val. Epoch: 48/100. Classification loss: 0.29165. Regression loss: 0.75790. Total loss: 1.04955\n",
      "Step: 99999. Epoch: 49/100. Iteration: 2000/2000. Cls loss: 0.05746. Reg loss: 0checkpoint...\n",
      "Step: 99999. Epoch: 49/100. Iteration: 2000/2000. Cls loss: 0.05746. Reg loss: 0\n",
      "Val. Epoch: 49/100. Classification loss: 0.24996. Regression loss: 0.70807. Total loss: 0.95803\n",
      "Step: 101999. Epoch: 50/100. Iteration: 2000/2000. Cls loss: 0.02978. Reg loss: checkpoint...\n",
      "Step: 101999. Epoch: 50/100. Iteration: 2000/2000. Cls loss: 0.02978. Reg loss: \n",
      "Val. Epoch: 50/100. Classification loss: 0.26419. Regression loss: 0.75938. Total loss: 1.02357\n",
      "Step: 103999. Epoch: 51/100. Iteration: 2000/2000. Cls loss: 0.00764. Reg loss: checkpoint...\n",
      "Step: 103999. Epoch: 51/100. Iteration: 2000/2000. Cls loss: 0.00764. Reg loss: \n",
      "Val. Epoch: 51/100. Classification loss: 0.25945. Regression loss: 0.69312. Total loss: 0.95257\n",
      "Step: 105999. Epoch: 52/100. Iteration: 2000/2000. Cls loss: 0.01780. Reg loss: checkpoint...\n",
      "Step: 105999. Epoch: 52/100. Iteration: 2000/2000. Cls loss: 0.01780. Reg loss: \n",
      "Val. Epoch: 52/100. Classification loss: 0.29753. Regression loss: 0.74749. Total loss: 1.04502\n",
      "Step: 107999. Epoch: 53/100. Iteration: 2000/2000. Cls loss: 0.03061. Reg loss: checkpoint...\n",
      "Step: 107999. Epoch: 53/100. Iteration: 2000/2000. Cls loss: 0.03061. Reg loss: \n",
      "Val. Epoch: 53/100. Classification loss: 0.25067. Regression loss: 0.71702. Total loss: 0.96769\n",
      "Step: 109999. Epoch: 54/100. Iteration: 2000/2000. Cls loss: 0.03049. Reg loss: checkpoint...\n",
      "Step: 109999. Epoch: 54/100. Iteration: 2000/2000. Cls loss: 0.03049. Reg loss: \n",
      "Val. Epoch: 54/100. Classification loss: 0.25242. Regression loss: 0.72286. Total loss: 0.97527\n",
      "Step: 111999. Epoch: 55/100. Iteration: 2000/2000. Cls loss: 0.00159. Reg loss: checkpoint...\n",
      "Step: 111999. Epoch: 55/100. Iteration: 2000/2000. Cls loss: 0.00159. Reg loss: \n",
      "Val. Epoch: 55/100. Classification loss: 0.27766. Regression loss: 0.72467. Total loss: 1.00233\n",
      "Step: 113999. Epoch: 56/100. Iteration: 2000/2000. Cls loss: 0.05163. Reg loss: checkpoint...\n",
      "Step: 113999. Epoch: 56/100. Iteration: 2000/2000. Cls loss: 0.05163. Reg loss: \n",
      "Val. Epoch: 56/100. Classification loss: 0.26213. Regression loss: 0.74852. Total loss: 1.01065\n",
      "Step: 115999. Epoch: 57/100. Iteration: 2000/2000. Cls loss: 0.06106. Reg loss: checkpoint...\n",
      "Step: 115999. Epoch: 57/100. Iteration: 2000/2000. Cls loss: 0.06106. Reg loss: \n",
      "Val. Epoch: 57/100. Classification loss: 0.22399. Regression loss: 0.74698. Total loss: 0.97097\n",
      "Step: 117999. Epoch: 58/100. Iteration: 2000/2000. Cls loss: 0.05577. Reg loss: checkpoint...\n",
      "Step: 117999. Epoch: 58/100. Iteration: 2000/2000. Cls loss: 0.05577. Reg loss: \n",
      "Val. Epoch: 58/100. Classification loss: 0.24455. Regression loss: 0.81686. Total loss: 1.06141\n",
      "Step: 119999. Epoch: 59/100. Iteration: 2000/2000. Cls loss: 0.04721. Reg loss: checkpoint...\n",
      "Step: 119999. Epoch: 59/100. Iteration: 2000/2000. Cls loss: 0.04721. Reg loss: \n",
      "Epoch 00012: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Val. Epoch: 59/100. Classification loss: 0.27587. Regression loss: 0.72321. Total loss: 0.99908\n",
      "Step: 121999. Epoch: 60/100. Iteration: 2000/2000. Cls loss: 0.01840. Reg loss: checkpoint...\n",
      "Step: 121999. Epoch: 60/100. Iteration: 2000/2000. Cls loss: 0.01840. Reg loss: \n",
      "Val. Epoch: 60/100. Classification loss: 0.26848. Regression loss: 0.63874. Total loss: 0.90722\n",
      "Step: 123999. Epoch: 61/100. Iteration: 2000/2000. Cls loss: 0.04034. Reg loss: checkpoint...\n",
      "Step: 123999. Epoch: 61/100. Iteration: 2000/2000. Cls loss: 0.04034. Reg loss: \n",
      "Val. Epoch: 61/100. Classification loss: 0.28942. Regression loss: 0.62606. Total loss: 0.91548\n",
      "Step: 125999. Epoch: 62/100. Iteration: 2000/2000. Cls loss: 0.01064. Reg loss: checkpoint...\n",
      "Step: 125999. Epoch: 62/100. Iteration: 2000/2000. Cls loss: 0.01064. Reg loss: \n",
      "Val. Epoch: 62/100. Classification loss: 0.28798. Regression loss: 0.62202. Total loss: 0.91001\n",
      "Step: 127999. Epoch: 63/100. Iteration: 2000/2000. Cls loss: 0.00910. Reg loss: checkpoint...\n",
      "Step: 127999. Epoch: 63/100. Iteration: 2000/2000. Cls loss: 0.00910. Reg loss: \n",
      "Val. Epoch: 63/100. Classification loss: 0.29601. Regression loss: 0.61884. Total loss: 0.91485\n",
      "Step: 129999. Epoch: 64/100. Iteration: 2000/2000. Cls loss: 0.03486. Reg loss: checkpoint...\n",
      "Step: 129999. Epoch: 64/100. Iteration: 2000/2000. Cls loss: 0.03486. Reg loss: \n",
      "Val. Epoch: 64/100. Classification loss: 0.28588. Regression loss: 0.61852. Total loss: 0.90440\n",
      "Step: 131999. Epoch: 65/100. Iteration: 2000/2000. Cls loss: 0.04382. Reg loss: checkpoint...\n",
      "Step: 131999. Epoch: 65/100. Iteration: 2000/2000. Cls loss: 0.04382. Reg loss: \n",
      "Val. Epoch: 65/100. Classification loss: 0.31017. Regression loss: 0.61510. Total loss: 0.92527\n",
      "Step: 133999. Epoch: 66/100. Iteration: 2000/2000. Cls loss: 0.09815. Reg loss: checkpoint...\n",
      "Step: 133999. Epoch: 66/100. Iteration: 2000/2000. Cls loss: 0.09815. Reg loss: \n",
      "Val. Epoch: 66/100. Classification loss: 0.32071. Regression loss: 0.61963. Total loss: 0.94034\n",
      "Step: 135999. Epoch: 67/100. Iteration: 2000/2000. Cls loss: 0.00971. Reg loss: checkpoint...\n",
      "Step: 135999. Epoch: 67/100. Iteration: 2000/2000. Cls loss: 0.00971. Reg loss: \n",
      "Val. Epoch: 67/100. Classification loss: 0.30519. Regression loss: 0.61578. Total loss: 0.92097\n",
      "Step: 137999. Epoch: 68/100. Iteration: 2000/2000. Cls loss: 0.00186. Reg loss: checkpoint...\n",
      "Step: 137999. Epoch: 68/100. Iteration: 2000/2000. Cls loss: 0.00186. Reg loss: \n",
      "Val. Epoch: 68/100. Classification loss: 0.32018. Regression loss: 0.61948. Total loss: 0.93966\n",
      "Step: 139999. Epoch: 69/100. Iteration: 2000/2000. Cls loss: 0.01737. Reg loss: checkpoint...\n",
      "Step: 139999. Epoch: 69/100. Iteration: 2000/2000. Cls loss: 0.01737. Reg loss: \n",
      "Val. Epoch: 69/100. Classification loss: 0.32868. Regression loss: 0.62084. Total loss: 0.94951\n",
      "Step: 141999. Epoch: 70/100. Iteration: 2000/2000. Cls loss: 0.00344. Reg loss: checkpoint...\n",
      "Step: 141999. Epoch: 70/100. Iteration: 2000/2000. Cls loss: 0.00344. Reg loss: \n",
      "Val. Epoch: 70/100. Classification loss: 0.32516. Regression loss: 0.61606. Total loss: 0.94122\n",
      "Step: 143999. Epoch: 71/100. Iteration: 2000/2000. Cls loss: 0.00740. Reg loss: checkpoint...\n",
      "Step: 143999. Epoch: 71/100. Iteration: 2000/2000. Cls loss: 0.00740. Reg loss: \n",
      "Val. Epoch: 71/100. Classification loss: 0.34336. Regression loss: 0.61225. Total loss: 0.95561\n",
      "Step: 145999. Epoch: 72/100. Iteration: 2000/2000. Cls loss: 0.06850. Reg loss: checkpoint...\n",
      "Step: 145999. Epoch: 72/100. Iteration: 2000/2000. Cls loss: 0.06850. Reg loss: \n",
      "Val. Epoch: 72/100. Classification loss: 0.34543. Regression loss: 0.61327. Total loss: 0.95870\n",
      "Step: 147999. Epoch: 73/100. Iteration: 2000/2000. Cls loss: 0.00423. Reg loss: checkpoint...\n",
      "Step: 147999. Epoch: 73/100. Iteration: 2000/2000. Cls loss: 0.00423. Reg loss: \n",
      "Val. Epoch: 73/100. Classification loss: 0.35589. Regression loss: 0.61891. Total loss: 0.97480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 149999. Epoch: 74/100. Iteration: 2000/2000. Cls loss: 0.02520. Reg loss: checkpoint...\n",
      "Step: 149999. Epoch: 74/100. Iteration: 2000/2000. Cls loss: 0.02520. Reg loss: \n",
      "Val. Epoch: 74/100. Classification loss: 0.36734. Regression loss: 0.61531. Total loss: 0.98265\n",
      "Step: 151999. Epoch: 75/100. Iteration: 2000/2000. Cls loss: 0.02414. Reg loss: checkpoint...\n",
      "Step: 151999. Epoch: 75/100. Iteration: 2000/2000. Cls loss: 0.02414. Reg loss: \n",
      "Val. Epoch: 75/100. Classification loss: 0.35608. Regression loss: 0.61310. Total loss: 0.96917\n",
      "Step: 153999. Epoch: 76/100. Iteration: 2000/2000. Cls loss: 0.03287. Reg loss: checkpoint...\n",
      "Step: 153999. Epoch: 76/100. Iteration: 2000/2000. Cls loss: 0.03287. Reg loss: \n",
      "Val. Epoch: 76/100. Classification loss: 0.36420. Regression loss: 0.61730. Total loss: 0.98150\n",
      "Step: 155999. Epoch: 77/100. Iteration: 2000/2000. Cls loss: 0.02204. Reg loss: checkpoint...\n",
      "Step: 155999. Epoch: 77/100. Iteration: 2000/2000. Cls loss: 0.02204. Reg loss: \n",
      "Val. Epoch: 77/100. Classification loss: 0.38449. Regression loss: 0.62064. Total loss: 1.00512\n",
      "Step: 157999. Epoch: 78/100. Iteration: 2000/2000. Cls loss: 0.00190. Reg loss: checkpoint...\n",
      "Step: 157999. Epoch: 78/100. Iteration: 2000/2000. Cls loss: 0.00190. Reg loss: \n",
      "Val. Epoch: 78/100. Classification loss: 0.38969. Regression loss: 0.62194. Total loss: 1.01163\n",
      "Step: 159999. Epoch: 79/100. Iteration: 2000/2000. Cls loss: 0.00445. Reg loss: checkpoint...\n",
      "Step: 159999. Epoch: 79/100. Iteration: 2000/2000. Cls loss: 0.00445. Reg loss: \n",
      "Val. Epoch: 79/100. Classification loss: 0.39438. Regression loss: 0.61895. Total loss: 1.01333\n",
      "Step: 161999. Epoch: 80/100. Iteration: 2000/2000. Cls loss: 0.00594. Reg loss: checkpoint...\n",
      "Step: 161999. Epoch: 80/100. Iteration: 2000/2000. Cls loss: 0.00594. Reg loss: \n",
      "Val. Epoch: 80/100. Classification loss: 0.39524. Regression loss: 0.62033. Total loss: 1.01556\n",
      "Step: 163999. Epoch: 81/100. Iteration: 2000/2000. Cls loss: 0.01731. Reg loss: checkpoint...\n",
      "Step: 163999. Epoch: 81/100. Iteration: 2000/2000. Cls loss: 0.01731. Reg loss: \n",
      "Val. Epoch: 81/100. Classification loss: 0.40650. Regression loss: 0.62133. Total loss: 1.02783\n",
      "Step: 165999. Epoch: 82/100. Iteration: 2000/2000. Cls loss: 0.00984. Reg loss: checkpoint...\n",
      "Step: 165999. Epoch: 82/100. Iteration: 2000/2000. Cls loss: 0.00984. Reg loss: \n",
      "Val. Epoch: 82/100. Classification loss: 0.40138. Regression loss: 0.62049. Total loss: 1.02187\n",
      "Step: 167999. Epoch: 83/100. Iteration: 2000/2000. Cls loss: 0.01033. Reg loss: checkpoint...\n",
      "Step: 167999. Epoch: 83/100. Iteration: 2000/2000. Cls loss: 0.01033. Reg loss: \n",
      "Val. Epoch: 83/100. Classification loss: 0.43747. Regression loss: 0.61530. Total loss: 1.05277\n",
      "Step: 169999. Epoch: 84/100. Iteration: 2000/2000. Cls loss: 0.00395. Reg loss: checkpoint...\n",
      "Step: 169999. Epoch: 84/100. Iteration: 2000/2000. Cls loss: 0.00395. Reg loss: \n",
      "Val. Epoch: 84/100. Classification loss: 0.42149. Regression loss: 0.62650. Total loss: 1.04800\n",
      "Step: 171999. Epoch: 85/100. Iteration: 2000/2000. Cls loss: 0.00230. Reg loss: checkpoint...\n",
      "Step: 171999. Epoch: 85/100. Iteration: 2000/2000. Cls loss: 0.00230. Reg loss: \n",
      "Val. Epoch: 85/100. Classification loss: 0.41788. Regression loss: 0.61924. Total loss: 1.03712\n",
      "Step: 173999. Epoch: 86/100. Iteration: 2000/2000. Cls loss: 0.00020. Reg loss: checkpoint...\n",
      "Step: 173999. Epoch: 86/100. Iteration: 2000/2000. Cls loss: 0.00020. Reg loss: \n",
      "Val. Epoch: 86/100. Classification loss: 0.45855. Regression loss: 0.62410. Total loss: 1.08265\n",
      "Step: 175999. Epoch: 87/100. Iteration: 2000/2000. Cls loss: 0.05425. Reg loss: checkpoint...\n",
      "Step: 175999. Epoch: 87/100. Iteration: 2000/2000. Cls loss: 0.05425. Reg loss: \n",
      "Val. Epoch: 87/100. Classification loss: 0.44466. Regression loss: 0.62280. Total loss: 1.06746\n",
      "Step: 177999. Epoch: 88/100. Iteration: 2000/2000. Cls loss: 0.02260. Reg loss: checkpoint...\n",
      "Step: 177999. Epoch: 88/100. Iteration: 2000/2000. Cls loss: 0.02260. Reg loss: \n",
      "Val. Epoch: 88/100. Classification loss: 0.45466. Regression loss: 0.62188. Total loss: 1.07654\n",
      "Step: 179999. Epoch: 89/100. Iteration: 2000/2000. Cls loss: 0.00593. Reg loss: checkpoint...\n",
      "Step: 179999. Epoch: 89/100. Iteration: 2000/2000. Cls loss: 0.00593. Reg loss: \n",
      "Val. Epoch: 89/100. Classification loss: 0.44884. Regression loss: 0.62651. Total loss: 1.07535\n",
      "Step: 181999. Epoch: 90/100. Iteration: 2000/2000. Cls loss: 0.02086. Reg loss: checkpoint...\n",
      "Step: 181999. Epoch: 90/100. Iteration: 2000/2000. Cls loss: 0.02086. Reg loss: \n",
      "Val. Epoch: 90/100. Classification loss: 0.43751. Regression loss: 0.62417. Total loss: 1.06168\n",
      "Step: 183999. Epoch: 91/100. Iteration: 2000/2000. Cls loss: 0.00161. Reg loss: checkpoint...\n",
      "Step: 183999. Epoch: 91/100. Iteration: 2000/2000. Cls loss: 0.00161. Reg loss: \n",
      "Val. Epoch: 91/100. Classification loss: 0.44397. Regression loss: 0.63140. Total loss: 1.07537\n",
      "Step: 185999. Epoch: 92/100. Iteration: 2000/2000. Cls loss: 0.02967. Reg loss: checkpoint...\n",
      "Step: 185999. Epoch: 92/100. Iteration: 2000/2000. Cls loss: 0.02967. Reg loss: \n",
      "Val. Epoch: 92/100. Classification loss: 0.45350. Regression loss: 0.62483. Total loss: 1.07833\n",
      "Step: 187999. Epoch: 93/100. Iteration: 2000/2000. Cls loss: 0.00208. Reg loss: checkpoint...\n",
      "Step: 187999. Epoch: 93/100. Iteration: 2000/2000. Cls loss: 0.00208. Reg loss: \n",
      "Val. Epoch: 93/100. Classification loss: 0.47763. Regression loss: 0.62553. Total loss: 1.10316\n",
      "Step: 189999. Epoch: 94/100. Iteration: 2000/2000. Cls loss: 0.00465. Reg loss: checkpoint...\n",
      "Step: 189999. Epoch: 94/100. Iteration: 2000/2000. Cls loss: 0.00465. Reg loss: \n",
      "Val. Epoch: 94/100. Classification loss: 0.46373. Regression loss: 0.62350. Total loss: 1.08724\n",
      "Step: 191999. Epoch: 95/100. Iteration: 2000/2000. Cls loss: 0.00692. Reg loss: checkpoint...\n",
      "Step: 191999. Epoch: 95/100. Iteration: 2000/2000. Cls loss: 0.00692. Reg loss: \n",
      "Val. Epoch: 95/100. Classification loss: 0.49173. Regression loss: 0.62454. Total loss: 1.11628\n",
      "Step: 193999. Epoch: 96/100. Iteration: 2000/2000. Cls loss: 0.00422. Reg loss: checkpoint...\n",
      "Step: 193999. Epoch: 96/100. Iteration: 2000/2000. Cls loss: 0.00422. Reg loss: \n",
      "Val. Epoch: 96/100. Classification loss: 0.47913. Regression loss: 0.62255. Total loss: 1.10168\n",
      "Step: 195999. Epoch: 97/100. Iteration: 2000/2000. Cls loss: 0.00435. Reg loss: checkpoint...\n",
      "Step: 195999. Epoch: 97/100. Iteration: 2000/2000. Cls loss: 0.00435. Reg loss: \n",
      "Val. Epoch: 97/100. Classification loss: 0.48286. Regression loss: 0.63300. Total loss: 1.11586\n",
      "Step: 197999. Epoch: 98/100. Iteration: 2000/2000. Cls loss: 0.00879. Reg loss: checkpoint...\n",
      "Step: 197999. Epoch: 98/100. Iteration: 2000/2000. Cls loss: 0.00879. Reg loss: \n",
      "Val. Epoch: 98/100. Classification loss: 0.46989. Regression loss: 0.62841. Total loss: 1.09829\n",
      "Step: 199999. Epoch: 99/100. Iteration: 2000/2000. Cls loss: 0.00628. Reg loss: checkpoint...\n",
      "Step: 199999. Epoch: 99/100. Iteration: 2000/2000. Cls loss: 0.00628. Reg loss: \n",
      "Val. Epoch: 99/100. Classification loss: 0.49546. Regression loss: 0.62429. Total loss: 1.11975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Failed to sample metric: process no longer exists (pid=2302)\n"
     ]
    }
   ],
   "source": [
    "!python train.py -p 'EfficientDet_d0' \\  # ÌîÑÎ°úÏ†ùÌä∏ Ïù¥Î¶Ñ\n",
    "-c 0 \\  # efficientdet Î≤ÑÏ†Ñ\n",
    "-n 2 \\  # workers\n",
    "--batch_size 4 \\  \n",
    "--lr 4e-4 \\\n",
    "--optim 'adamw' \\\n",
    "--num_epochs 100 \\\n",
    "--save_interval 2000 \\  # iteration Î™á Î≤àÎßàÎã§ ckpt Ï†ÄÏû•Ìï†ÏßÄ\n",
    "--saved_path 'EfficientDet_ckpt' \\  # ckpt Ï†ÄÏû• ÏúÑÏπò\n",
    "--load_weights EfficientDet_ckpt  # pretrained_ckptÎÇò Ïù¥Ï†ÑÏùò ckpt Í≤ΩÎ°ú"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
